import asyncio
import argparse
import csv
import datetime
import json
import os
import re
import sys
import time
from statistics import mean
from typing import Any, Dict, List, Optional, Union
from urllib.parse import urlparse
from dotenv import load_dotenv
import numpy as np
import openai
import tiktoken
import yaml
from pydantic import BaseModel, Field, field_validator
from tabulate import tabulate
from tqdm import tqdm
import warnings

os.environ["TOKENIZERS_PARALLELISM"] = "false"

try:
    from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast
except ImportError:
    AutoTokenizer = None
    PreTrainedTokenizer = None
    PreTrainedTokenizerFast = None


class ModelConfig(BaseModel):
    """Configuration for a specific model.

    All fields are optional and have default values.

    Attributes:
        tokenizer_type: The type of tokenizer to use, e.g., "huggingface" or "tiktoken".
        tokenizer: The specific tokenizer name or path.
        n_ctx: The context size for the model.
        max_tokens: The default maximum number of tokens for a completion.
        truncate: Whether to truncate the context if it exceeds the model's context window.
        temperature: The default sampling temperature.
    """

    tokenizer_type: str = "tiktoken"
    tokenizer: Optional[str] = None
    n_ctx: Optional[int] = None
    max_tokens: Optional[int] = None
    truncate: bool = False
    temperature: float = 0.0


class ResultRecord(BaseModel):
    """Represents a single record of a test query result.

    Attributes:
        record_id: The unique identifier for the record.
        model_name: The name of the model being tested.
        messages: The list of messages sent to the model.
        success: A boolean indicating if the query was successful.
        is_batch: A boolean indicating if the query was done in batch mode.
        temperature: The sampling temperature used for the query.
        max_tokens: The maximum number of tokens for the generation.
        system_prompt: The system prompt used.
        user_prompt_template: The user prompt template used.
        usage_exists: Whether the usage payload was present in the response.
        computed_prompt_tokens: The number of prompt tokens calculated by the tokenizer.
        computed_generation_tokens: The number of generation tokens calculated by the tokenizer.
        computed_total_tokens: The total number of tokens calculated by the tokenizer.
        computed_tokens_per_second: Tokens per second calculated using the tokenizer.
        generated_text: The text generated by the model.
        prompt_tokens: The number of tokens in the prompt from the usage payload.
        generation_tokens: The number of tokens in the generated text from the usage payload.
        total_tokens: The total number of tokens (prompt + generation) from the usage payload.
        latency_sec: The total time taken for the query.
        ttft_sec: The time to the first token.
        tokens_per_second: The number of tokens generated per second.
        total_tps: The total tokens per second, including prompt processing.
        error: The error message if the query failed.
    """

    record_id: str
    model_name: str
    messages: List[Dict[str, str]]
    success: bool
    is_batch: bool = False
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    system_prompt: Optional[str] = None
    user_prompt_template: Optional[str] = None
    usage_exists: bool = Field(default=False)
    computed_prompt_tokens: Optional[int] = None
    computed_generation_tokens: Optional[int] = None
    computed_total_tokens: Optional[int] = None
    computed_tokens_per_second: Optional[float] = None
    generated_text: Optional[str] = None
    prompt_tokens: Optional[int] = None
    generation_tokens: Optional[int] = None
    total_tokens: Optional[int] = None
    latency_sec: Optional[float] = None
    ttft_sec: Optional[float] = None
    tokens_per_second: Optional[float] = None
    total_tps: Optional[float] = None
    error: Optional[str] = None

    @field_validator("generated_text", mode="before")
    @classmethod
    def warn_if_empty(cls, v):
        if v is None or str(v).strip() == "":
            warnings.warn("Warning: 'generated_text' is optional but empty or missing.")
        return v


    def model_dump_json(self, **kwargs):
        """Dump the model to a JSON string, excluding None values."""
        return super().model_dump(exclude_none=True, **kwargs).to_json()


def build_output_filename(
    api_base: Optional[str], model_name: str, feature_name: str
) -> str:
    """Build a sanitized, timestamped filename for the output.

    Args:
        api_base: The base URL of the API.
        model_name: The name of the model being tested.
        feature_name: The name of the feature being tested.

    Returns:
        A sanitized filename string.
    """
    domain = "default_openai"
    if api_base:
        try:
            parsed_url = urlparse(api_base)
            domain = parsed_url.netloc
        except Exception:
            domain = "unknown_host"

    sanitized_domain = re.sub(r"[^\w.-]+", "_", domain)
    sanitized_model_name = re.sub(r"[^\w.-]+", "_", model_name)
    sanitized_feature_name = re.sub(r"[^\w.-]+", "_", feature_name)
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    return f"{sanitized_domain}_{sanitized_model_name}_{sanitized_feature_name}_{timestamp}.jsonl"


class OutputBuilder:
    """Builds and manages the output file for test results."""

    def __init__(
        self,
        model_name: str,
        feature_name: str,
        api_base: Optional[str],
        output_dir: Optional[str] = None,
    ):
        """Initializes the OutputBuilder.

        Args:
            model_name: The name of the model.
            feature_name: The name of the feature.
            api_base: The API base URL.
            output_dir: The directory to save the output file.
        """
        self.output_filename = build_output_filename(
            api_base, model_name, feature_name
        )
        if output_dir:
            self.output_path = os.path.join(output_dir, self.output_filename)
            os.makedirs(output_dir, exist_ok=True)
        else:
            self.output_path = self.output_filename

    def record(self, record_data: ResultRecord):
        """Writes a result record to the output file.

        Args:
            record_data: The result record to write.
        """
        with open(self.output_path, "a") as f:
            f.write(json.dumps(record_data.model_dump(exclude_none=True)) + "\n")


def load_config(config_path: str = "src/smoke/config.yaml") -> dict:
    """Load the YAML configuration file.

    Args:
        config_path: The path to the configuration file.

    Returns:
        A dictionary containing the configuration.
    """
    try:
        with open(config_path, "r") as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        print(f"Error: Configuration file not found at {config_path}")
        sys.exit(1)
    except yaml.YAMLError as e:
        print(f"Error parsing YAML file: {e}")
        sys.exit(1)


def get_vendor_config(vendor: str, config: dict) -> dict:
    """Get the configuration for a specific vendor.

    Args:
        vendor: The name of the vendor.
        config: The main configuration dictionary.

    Returns:
        A dictionary containing the vendor's configuration.
    """
    vendor_config = config.get("vendors", {}).get(vendor)
    if not vendor_config:
        print(f"Error: Vendor '{vendor}' not found in the configuration.")
        sys.exit(1)

    api_key_env = vendor_config.get("api_key_env")
    api_key = os.getenv(api_key_env) if api_key_env else None

    if not api_key:
        print(
            f"Error: API key environment variable '{api_key_env}' not set for vendor '{vendor}'."
        )
        sys.exit(1)

    vendor_config["api_key"] = api_key
    return vendor_config


def setup_tokenizer(
    model_name: str, model_config: ModelConfig
) -> Union[PreTrainedTokenizer, PreTrainedTokenizerFast, Any]:
    """Set up the tokenizer for a given model.

    Args:
        model_name: The name of the model.
        model_config: The configuration for the model.

    Returns:
        A tokenizer instance.
    """
    if AutoTokenizer is None:
        raise ImportError(
            "`transformers` library is not installed. Please install it with `pip install transformers`."
        )
    if model_config.tokenizer_type == "huggingface":
        tokenizer_name = model_config.tokenizer or model_name
        return AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=False)
    else:  # tiktoken
        try:
            return tiktoken.encoding_for_model(model_name)
        except KeyError:
            return tiktoken.get_encoding("cl100k_base")


def truncate_context(
    user_content: str,
    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast, Any],
    model_config: ModelConfig,
    system_prompt: str,
    max_tokens: int,
) -> str:
    """Truncate the user_content to fit within the model's context window.

    Args:
        user_content: The user content to truncate.
        tokenizer: The tokenizer to use for encoding.
        model_config: The model's configuration.
        system_prompt: The system prompt.
        max_tokens: The maximum number of tokens for the generation.

    Returns:
        The truncated context.
    """
    if model_config.truncate and max_tokens is None:
        raise ValueError(
            "Cannot truncate context: truncate=True but max_tokens not provided. "
            "Please set max_tokens in model config or disable truncate."
        )

    if not model_config.truncate:
        return user_content

    system_tokens_length = len(tokenizer.encode(system_prompt))
    available = max_tokens - (system_tokens_length + 50)

    if model_config.n_ctx:
        available = min(
            available, model_config.n_ctx - (system_tokens_length + max_tokens + 50)
        )

    encoded_context = tokenizer.encode(user_content)
    if len(encoded_context) > available:
        user_content = tokenizer.decode(encoded_context[:available])

    return user_content


async def run_query(
    session_id: int,
    query_id: int,
    prompt_data: dict,
    stats: List[dict],
    model_name: str,
    openai_client: openai.AsyncOpenAI,
    tokenizer: Optional[Any],
    system_prompt: str,
    user_prompt_template: str,
    model_config: ModelConfig,
    temperature: float,
    max_tokens: int,
    record_id: str,
    pbar: Optional[tqdm] = None,
    output_builder: Optional[OutputBuilder] = None,
):
    """Run a single query against the model.

    Args:
        session_id: The ID of the current session.
        query_id: The ID of the current query.
        prompt_data: The data to format into the prompt.
        stats: A list to store statistics.
        model_name: The name of the model to use.
        openai_client: The OpenAI async client.
        tokenizer: The tokenizer to use for token counting.
        system_prompt: The system prompt.
        user_prompt_template: The user prompt template.
        model_config: The configuration for the model.
        temperature: The sampling temperature.
        max_tokens: The maximum number of tokens to generate.
        record_id: The unique ID for this record.
        pbar: The progress bar instance.
        output_builder: The output builder instance.
    """
    user_content = user_prompt_template.format(**prompt_data)

    if model_config.truncate:
        user_content = truncate_context(
            user_content,
            tokenizer,
            model_config,
            system_prompt,
            max_tokens,
        )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content},
    ]

    try:
        start_time = time.time()
        first_token_time = None
        generated_text = ""
        usage = None
        is_batch = False
        stream = await openai_client.chat.completions.create(
            model=model_name,
            messages=messages,
            stream=True,
            max_tokens=max_tokens,
            temperature=temperature,
            stream_options={"include_usage": True},
        )

        async for chunk in stream:
            if chunk.usage:
                usage = chunk.usage
                continue

            if chunk.choices:
                if not first_token_time:
                    first_token_time = time.time()
                content = chunk.choices[0].delta.content or ""
                generated_text += content

        if not generated_text:
            is_batch = True
            completion = await openai_client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    stream=False,
                    max_tokens=max_tokens,
                    temperature=temperature,
                )
            if completion.choices:
                generated_text = completion.choices[0].message.content
            if completion.usage:
                usage = completion.usage

        end_time = time.time()
        total_time = end_time - start_time

        usage_exists = usage is not None

        prompt_tokens = -1
        completion_tokens = -1
        total_tokens = -1

        if usage_exists:
            prompt_tokens = usage.prompt_tokens if usage.prompt_tokens is not None else -1
            completion_tokens = (
                usage.completion_tokens if usage.completion_tokens is not None else -1
            )
            total_tokens = usage.total_tokens if usage.total_tokens is not None else -1

        computed_prompt_tokens = -1
        computed_completion_tokens = -1
        computed_total_tokens = -1

        if tokenizer and hasattr(tokenizer, "encode"):
            computed_prompt_tokens = len(tokenizer.encode(system_prompt)) + len(
                tokenizer.encode(user_content)
            )
            computed_completion_tokens = len(tokenizer.encode(generated_text))
            computed_total_tokens = (
                computed_prompt_tokens + computed_completion_tokens
            )

        tokens_for_tps = (
            completion_tokens
            if usage_exists and completion_tokens > -1
            else computed_completion_tokens
        )
        total_tokens_for_tps = (
            total_tokens
            if usage_exists and total_tokens > -1
            else computed_total_tokens
        )

        tps = (
            tokens_for_tps / total_time
            if total_time > 0 and tokens_for_tps > 0
            else 0
        )
        total_tps = (
            total_tokens_for_tps / total_time
            if total_time > 0 and total_tokens_for_tps > 0
            else 0
        )
        computed_tps = (
            computed_completion_tokens / total_time
            if total_time > 0 and computed_completion_tokens > 0
            else 0
        )

        stats.append(
            {
                "session": session_id,
                "query": query_id,
                "ttft": first_token_time - start_time if first_token_time else None,
                "tps": tps,
                "total_tps": total_tps,
                "success": True,
                "total_time": total_time,
                "output_tokens": tokens_for_tps,
            }
        )
        if output_builder:
            record = ResultRecord(
                record_id=record_id,
                model_name=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                messages=messages,
                system_prompt=system_prompt,
                user_prompt_template=user_prompt_template,
                usage_exists=usage_exists,
                computed_prompt_tokens=computed_prompt_tokens,
                computed_generation_tokens=computed_completion_tokens,
                computed_total_tokens=computed_total_tokens,
                computed_tokens_per_second=computed_tps,
                generated_text=generated_text,
                prompt_tokens=prompt_tokens,
                generation_tokens=completion_tokens,
                total_tokens=total_tokens,
                latency_sec=total_time,
                ttft_sec=first_token_time - start_time
                if first_token_time
                else None,
                tokens_per_second=tps,
                total_tps=total_tps,
                success=True,
                is_batch=is_batch,
            )
            output_builder.record(record)

    except Exception as e:
        stats.append(
            {
                "session": session_id,
                "query": query_id,
                "error": str(e),
                "success": False,
            }
        )
        if output_builder:
            record = ResultRecord(
                record_id=record_id,
                model_name=model_name,
                messages=messages,
                success=False,
                error=str(e),
                is_batch=is_batch,
            )
            output_builder.record(record)

    if pbar:
        pbar.update(1)


def stats_summary(values: List[float], label: str) -> List[str]:
    """Generate a summary of statistics for a given list of values.

    Args:
        values: A list of float values.
        label: A label for the statistics.

    Returns:
        A list of strings representing the summary.
    """
    return (
        [
            label,
            f"{mean(values):.2f}",
            f"{np.percentile(values, 50):.2f}",
            f"{np.percentile(values, 90):.2f}",
        ]
        if values
        else [label, "-", "-", "-"]
    )


async def async_main(args: argparse.Namespace) -> int:
    """The main async function for running the quality test.

    Args:
        args: Command-line arguments.

    Returns:
        The number of failed queries.
    """
    tokenizer = None
    stats: List[dict] = []
    config = load_config()

    api_base: Optional[str] = None
    model_config: ModelConfig

    if args.vendor:
        vendor_config = get_vendor_config(args.vendor, config)
        api_base = vendor_config.get("api_base")
        client_kwargs = {"api_key": vendor_config["api_key"], "base_url": api_base}

        model_configs = vendor_config.get("model_config", {})
        model_config_dict = model_configs.get(args.model)

        if not model_config_dict:
            sys.exit(
                f"Error: Model '{args.model}' configuration not found for vendor '{args.vendor}'.\n"
                f"Please add it to the config.yaml file. Supported models are: {list(model_configs.keys())}"
            )
        model_config = ModelConfig(**model_config_dict)
        try:
            tokenizer = setup_tokenizer(args.model, model_config)
        except (ImportError, Exception) as e:
            sys.exit(f"Failed to setup tokenizer: {e}")

    else:
        api_base = args.api_base
        client_kwargs = {"api_key": args.api_key, "base_url": api_base}
        model_config = ModelConfig()
        # Heuristic for non-vendor case
        if api_base is None or "openai.com" in api_base:
            try:
                tokenizer = tiktoken.encoding_for_model(args.model)
            except KeyError:
                print(
                    "Warning: Model not found for tiktoken. Token counting will be skipped."
                )

    output_builder = OutputBuilder(args.model, args.feature, api_base, args.output)
    openai_client = openai.AsyncOpenAI(**client_kwargs)

    features = config.get("features", {})
    if args.feature not in features:
        print(f"Error: Feature '{args.feature}' not found in config.yaml.")
        sys.exit(1)

    prompts_config = features[args.feature]
    system_prompt = prompts_config["system_prompt_template"]
    user_prompt_template = prompts_config["user_prompt_template"]

    try:
        with open(args.quality_test_csv, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            fieldnames = reader.fieldnames or []
            prompt_columns = [
                c.strip() for c in args.quality_test_csv_column.split(",")
            ]

            for column in prompt_columns:
                if column not in fieldnames:
                    print(
                        f"Error: Column '{column}' not found in {args.quality_test_csv}"
                    )
                    print(f"Available columns: {fieldnames}")
                    return 1

            if args.record_id_column and args.record_id_column not in fieldnames:
                print(
                    f"Error: Column '{args.record_id_column}' not found in {args.quality_test_csv}"
                )
                print(f"Available columns: {fieldnames}")
                return 1
            csv_file_data = [
                {
                    "prompt_data": {col: row[col] for col in prompt_columns},
                    "record_id": row.get(args.record_id_column, str(i))
                    if args.record_id_column
                    else str(i),
                }
                for i, row in enumerate(reader)
            ]
    except FileNotFoundError:
        print(f"Error: The file {args.quality_test_csv} was not found.")
        return 1

    pbar = tqdm(total=len(csv_file_data), desc="Running quality test")
    semaphore = asyncio.Semaphore(args.num_users)

    async def run_quality_query(session_id, query_id, extracted_row_data):
        async with semaphore:
            await run_query(
                session_id,
                query_id,
                extracted_row_data["prompt_data"],
                stats,
                args.model,
                openai_client,
                tokenizer,
                system_prompt,
                user_prompt_template,
                model_config,
                args.temperature,
                args.max_tokens,
                extracted_row_data["record_id"],
                pbar,
                output_builder,
            )

    tasks = [
        run_quality_query(i % args.num_users, i, pdata)
        for i, pdata in enumerate(csv_file_data)
    ]
    await asyncio.gather(*tasks)
    pbar.close()

    success = all(entry.get("success", False) for entry in stats)
    ttf_times = [
        s["ttft"] for s in stats if s.get("success") and s.get("ttft") is not None
    ]
    per_query_tps = [s["tps"] for s in stats if s.get("success") and s.get("tps")]
    total_tps = [
        s["total_tps"]
        for s in stats
        if s.get("success") and s.get("total_tps")
    ]

    total_output_tokens = sum(
        s.get("output_tokens", 0) for s in stats if s.get("success")
    )
    total_duration = sum(s["total_time"] for s in stats if s.get("success"))
    global_tps = (
        total_output_tokens / total_duration
        if total_duration > 0 and total_output_tokens > 0
        else 0
    )

    total = len(stats)
    successes = sum(1 for s in stats if s.get("success"))
    failures = total - successes
    table = [
        stats_summary(ttf_times, "Time to First Token (s)"),
        stats_summary(per_query_tps, "Tokens/sec (Per Query)"),
        stats_summary(total_tps, "Total TPS (Per Query)"),
    ]

    errors = [s for s in stats if not s.get("success") and s.get("error")]

    print("\n--- SUMMARY REPORT ---")
    print(f"Total Queries: {total}")
    print(f"Successful Queries: {successes}")
    print(f"Failed Queries: {failures}")
    print(tabulate(table, headers=["Metric", "Mean", "P50", "P90"], tablefmt="grid"))

    print(
        f"\nGlobal Throughput: {global_tps:.2f} tokens/sec across {total_duration:.2f} seconds"
    )
    if tokenizer is None:
        print(
            "Note: Token-based metrics (TPS, Global Throughput) are not available as a tokenizer could not be loaded."
        )
    print("SUCCESS" if success else "FAILURE: Some queries failed")

    if errors:
        print("\n--- FIRST ERROR ---")
        print(f"Session {errors[0]['session']} - Query {errors[0]['query']}")
        print(f"Error: {errors[0]['error']}")

    return failures


def main() -> int:
    """Parse command-line arguments and run the quality test.

    Returns:
        The exit code of the program.
    """
    load_dotenv()
    parser = argparse.ArgumentParser(description="OpenAI Quality Test Runner")
    parser.add_argument("--model", required=True, type=str, help="Model name")
    parser.add_argument(
        "--feature",
        type=str,
        default="default",
        help="The feature to test, which determines the system and user prompts.",
    )
    parser.add_argument(
        "--vendor",
        type=str,
        default=None,
        help="The vendor name. If specified, uses the vendor's API key and base URL from config.yaml.",
    )
    parser.add_argument(
        "--num-users", type=int, default=10, help="Number of concurrent workers"
    )
    parser.add_argument(
        "--temperature", type=float, default=0.7, help="The sampling temperature."
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=2000,
        help="The maximum number of tokens to generate.",
    )
    parser.add_argument(
        "--quality-test-csv",
        required=True,
        type=str,
        help="Path to the CSV file for quality testing.",
    )
    parser.add_argument(
        "--quality-test-csv-column",
        type=str,
        default="text",
        help="Comma-separated column names in the CSV file to use for the prompt.",
    )
    parser.add_argument(
        "--record-id-column",
        type=str,
        default=None,
        help="The column name in the CSV file that contains the record ID. If not provided, the row number is used.",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="Directory to save the output JSONL file. Defaults to current directory.",
    )
    parser.add_argument(
        "--api-key",
        type=str,
        default=None,
        help="API key. Cannot be used with --vendor.",
    )
    parser.add_argument(
        "--api-base",
        type=str,
        default=None,
        help="API base URL. Cannot be used with --vendor.",
    )

    args = parser.parse_args()

    if args.vendor:
        if args.api_key is not None or args.api_base is not None:
            parser.error("--api-key and --api-base cannot be used with --vendor.")
    elif args.api_key is None:
        # If no vendor, check for direct key or OPENAI_API_KEY
        env_api_key = os.getenv("OPENAI_API_KEY")
        if env_api_key is None:
            parser.error(
                "Either --vendor or --api-key (or OPENAI_API_KEY env var) is required."
            )
        args.api_key = env_api_key

    return asyncio.run(async_main(args))


if __name__ == "__main__":
    sys.exit(main())