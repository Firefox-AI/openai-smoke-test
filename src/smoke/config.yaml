features:
  default:
    system_prompt_template: "You are a helpful assistant."
    user_prompt_template: "{text}"
  summarization:
    use_dataset: false # Use dataset instead of lorem ipsum, setting this true also runs summarization scoring
    log_stats: false # Setting to true iterates through entire dataset and logs stats in stats.jsonl
    dataset_name: "page-summarization-eval" # (prefixed with Mozilla/) loaded from https://huggingface.co/Mozilla/datasets
    system_prompt_template: "You are an expert at creating mobile-optimized summaries. Process: Step 1: Identify the type of content. Step 2: Based on content type, prioritize: Recipe - Servings, Total time, Ingredients list, Key steps, Tips. News - What happened, when, where. How-to - Total time, Materials, Key steps, Warnings. Review - Bottom line rating, price. Opinion - Main arguments, Key evidence. Personal Blog - Author, main points. Fiction - Author, summary of plot. All other content types - Provide a brief summary of no more than 6 sentences. Step 3: Format for mobile using concise language and paragraphs with 3 sentences maximum. Bold critical details (numbers, warnings, key terms)."
    user_prompt_template: '''Summarize the following article in a single, dense paragraph. Do not use any markdown, headings, bullet points, or numbered lists. The summary should be presented as a single block of text. Article: {text}'''
    temperature: 0.1
    top_p: 0.01
    max_completion_tokens: null # max tokens for summarization call, set to 0 for None
    error_on_threshold_fails: false
    metric_threshold:
      ttft: 1 # must be lower (all others are: must be higher)
      per_query_tps: 100
      round_trip: 2.5 # must be lower
    score_threshold:
      rouge:
        rouge1: .3
        rouge2: .2
        rougeLsum: .25
      bleu: .1
      unieval:
        consistency: .9
        coherence: .8
        fluency: .8
        relevance: .75
        overall: .85
      percentage_of_hiccups: .05 # must be lower
      overall: .65
    stream: true # Stream chat completion and track ttft
    service_account_file: "creds.json" # service account credentials for gcloud auth mistral ai
    llm_unieval_scoring: # Replace unieval with equivalent scoring using LLM
      score_with_llm: false
      model_name: "gpt-4o"
      base_url: "https://api.openai.com/v1/"
      api_key: LLM_UNIEVAL_SCORING_API_KEY
      system_prompt: "You are a meticulous and impartial AI expert specializing in the evaluation of text summaries. Your analysis must be objective and based solely on the provided texts. You will score the summary on five specific criteria, providing your final output as a single, clean JSON object and nothing else."
      user_prompt: 'Carefully evaluate the "PREDICTION SUMMARY" against the "ORIGINAL CONTENT". Use the "REFERENCE SUMMARY" as a gold standard for what a high-quality summary should contain.\n\n### EVALUATION CRITERIA\nYou must rate the summary on a scale of 0.0 to 1.0 for each of the following five criteria:\n1.  **Consistency**: How factually accurate is the summary compared to the ORIGINAL CONTENT? (1.0 = fully consistent, 0.0 = completely contradictory)\n2.  **Coherence**: How well-structured and easy to understand is the summary? Does it flow logically? (1.0 = perfectly coherent, 0.0 = nonsensical)\n3.  **Fluency**: How well-written is the summary in terms of grammar, spelling, and style? (1.0 = flawless, 0.0 = ungrammatical)\n4.  **Relevance**: How well does the summary capture the main points of the ORIGINAL CONTENT, as guided by the REFERENCE SUMMARY? (1.0 = captures all key points, 0.0 = completely irrelevant)\n5.  **Overall**: Your holistic score reflecting the summarys overall quality and usefulness.\n\n### INPUT TEXTS\n**--- ORIGINAL CONTENT ---**\n{original_content}\n**--- REFERENCE SUMMARY ---**\n{reference}\n**--- PREDICTION SUMMARY ---**\n{prediction}\n\n### REQUIRED OUTPUT FORMAT\nProvide your evaluation as a single JSON object. Do not include any text before or after the JSON.\n\nExample:\n{\n  "consistency": 0.9,\n  "coherence": 1.0,\n  "fluency": 1.0,\n  "relevance": 0.8,\n  "overall": 0.9\n}'

vendors:
  openai:
    api_key_env: OPENAI_API_KEY
    api_base: null
    model_config:
      gpt-4o:
        tokenizer_type: "tiktoken"
      o4-mini:
        tokenizer_type: "tiktoken"
      gpt-4.1:
        tokenizer_type: "tiktoken"
      gpt-5-mini:
        tokenizer_type: "tiktoken"
      gpt-5-nano:
        tokenizer_type: "tiktoken"
      gpt-5:
        tokenizer_type: "tiktoken"

  together-ai:
    api_key_env: TOGETHER_AI_API_KEY
    api_base: https://api.together.xyz/v1
    model_config:
      "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo":
        tokenizer_type: "huggingface"
        tokenizer: "meta-llama/Llama-3.1-8B-Instruct"
      "meta-llama/Llama-3.3-70B-Instruct-Turbo":
        tokenizer_type: "huggingface"
        tokenizer: "meta-llama/Llama-3.3-70B-Instruct"
      "openai/gpt-oss-120B":
        tokenizer_type: "huggingface"
        tokenizer: "openai/gpt-oss-120b"
      "Qwen/Qwen3-235B-A22B-fp8-tput":
        tokenizer_type: "huggingface"
        tokenizer: "Qwen/Qwen3-235B-A22B-FP8"
      "Qwen/Qwen3-235B-A22B-Instruct-2507-tput": #built-in no-thinking model
        tokenizer_type: "huggingface"
        tokenizer: "Qwen/Qwen3-235B-A22B-Instruct-2507-FP8"
      "zai-org/GLM-4.5-Air-FP8":
        tokenizer_type: "huggingface"
        tokenizer: "zai-org/GLM-4.5-Air-FP8"
      "deepseek-ai/DeepSeek-R1":
        tokenizer_type: "huggingface"
        tokenizer: "deepseek-ai/DeepSeek-R1"

  groq:
    api_key_env: GROQ_API_KEY
    api_base: "https://api.groq.com/openai/v1"
    model_config:
      "llama-3.1-8b-instant":
        tokenizer_type: "huggingface"
        tokenizer: "meta-llama/Llama-3.1-8B-Instruct"
      "llama-3.3-70b-versatile":
        tokenizer_type: "huggingface"
        tokenizer: "meta-llama/Llama-3.3-70B-Instruct"
      "openai/gpt-oss-120b":
        tokenizer_type: "huggingface"
        tokenizer: "openai/gpt-oss-120b"
      "openai/gpt-oss-20b":
        tokenizer_type: "huggingface"
        tokenizer: "openai/gpt-oss-20b"
      "qwen/qwen3-32b":
        tokenizer_type: "huggingface"
        tokenizer: "Qwen/Qwen3-32B"
      "deepseek-r1-distill-llama-70b":
        tokenizer_type: "huggingface"
        tokenizer: "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"

  flower-ai:
    api_key_env: FLOWER_API_KEY
    api_base: "https://api.flower.ai/v1"
    model_config:
      "meta/llama3.1-8b/instruct-int4":
        tokenizer_type: "huggingface"
        tokenizer: "clowman/Llama-3.1-8B-Instruct-GPTQ-Int4"
      "meta/llama3.3-70b/instruct-int4":
        tokenizer_type: "huggingface"
        tokenizer: "Satwik11/Llama-3.3-70B-Instruct-AutoRound-GPTQ-4bit"
        truncate: true
        max_tokens: 1500

  cerebras:
    api_key_env: CEREBRAS_API_KEY
    api_base: "https://api.cerebras.ai/v1"
    model_config:
      "gpt-oss-120b":
        tokenizer_type: "huggingface"
        tokenizer: "openai/gpt-oss-120b"

  ollama:
    api_key_env: OLLAMA_API_KEY
    api_base: "http://localhost:11434/v1"
    model_config:
      "TheBloke/Mistral-7B-Instruct-v0.2-AWQ":
        uri: "http://localhost:8000/v1/chat/completions"
        temperature: 0.0
        max_tokens: 1000
        tokenizer_type: "huggingface"
        tokenizer: "mistralai/Mistral-7B-Instruct-v0.2"
      "Qwen/Qwen3-0.6B-GGUF":
        temperature: 0.0
        max_tokens: 1000
        truncate: true
        tokenizer_type: "huggingface"
        tokenizer: "Qwen/Qwen3-0.6B"
      "google/gemma-3-1b-it-qat-q4_0-gguf":
        temperature: 0.0
        max_tokens: 1000
        tokenizer_type: "huggingface"
        tokenizer: "google/gemma-3-1b-it"
      "Qwen/Qwen3-4B-Thinking-2507":
        temperature: 0.0
        max_tokens: 1000
        tokenizer_type: "huggingface"
        tokenizer: "Qwen/Qwen3-4B-Thinking-2507"
