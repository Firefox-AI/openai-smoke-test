features:
  default:
    system_prompt_template: "You are a helpful assistant."
    user_prompt_template: "{text}"
  summarization:
    system_prompt_template: "You are a helpful text assistant dedicated to providing accurate, respectful, and thoughtful assistance. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity."
    user_prompt_template: "'Summarize the following text using precise and concise language. Use headers and bulleted lists in the summary. Maintain the meaning and factual accuracy. # Article:  {text}'"

vendors:
  openai:
    api_key_env: OPENAI_API_KEY
    api_base: null
    model_config:
      gpt-4o:
        tokenizer_type: "tiktoken"
      o4-mini:
        tokenizer_type: "tiktoken"
      gpt-4.1:
        tokenizer_type: "tiktoken"
      gpt-5-mini:
        tokenizer_type: "tiktoken"
      gpt-5-nano:
        tokenizer_type: "tiktoken"
      gpt-5:
        tokenizer_type: "tiktoken"

  together-ai:
    api_key_env: TOGETHER_AI_API_KEY
    api_base: https://api.together.xyz/v1
    model_config:
      "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo":
        tokenizer_type: "huggingface"
        tokenizer: "meta-llama/Llama-3.1-8B-Instruct"
      "meta-llama/Llama-3.3-70B-Instruct-Turbo":
        tokenizer_type: "huggingface"
        tokenizer: "meta-llama/Llama-3.3-70B-Instruct"
      "openai/gpt-oss-120B":
        tokenizer_type: "huggingface"
        tokenizer: "openai/gpt-oss-120b"
      "Qwen/Qwen3-235B-A22B-fp8-tput":
        tokenizer_type: "huggingface"
        tokenizer: "Qwen/Qwen3-235B-A22B-FP8"
      "Qwen/Qwen3-235B-A22B-Instruct-2507-tput": #built-in no-thinking model
        tokenizer_type: "huggingface"
        tokenizer: "Qwen/Qwen3-235B-A22B-Instruct-2507-FP8"
      "zai-org/GLM-4.5-Air-FP8":
        tokenizer_type: "huggingface"
        tokenizer: "zai-org/GLM-4.5-Air-FP8"
      "deepseek-ai/DeepSeek-R1":
        tokenizer_type: "huggingface"
        tokenizer: "deepseek-ai/DeepSeek-R1"

  groq:
    api_key_env: GROQ_API_KEY
    api_base: "https://api.groq.com/openai/v1"
    model_config:
      "llama-3.1-8b-instant":
        tokenizer_type: "huggingface"
        tokenizer: "meta-llama/Llama-3.1-8B-Instruct"
      "llama-3.3-70b-versatile":
        tokenizer_type: "huggingface"
        tokenizer: "meta-llama/Llama-3.3-70B-Instruct"
      "openai/gpt-oss-120b":
        tokenizer_type: "huggingface"
        tokenizer: "openai/gpt-oss-120b"
      "openai/gpt-oss-20b":
        tokenizer_type: "huggingface"
        tokenizer: "openai/gpt-oss-20b"
      "qwen/qwen3-32b":
        tokenizer_type: "huggingface"
        tokenizer: "Qwen/Qwen3-32B"
      "deepseek-r1-distill-llama-70b":
        tokenizer_type: "huggingface"
        tokenizer: "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"

  flower-ai:
    api_key_env: FLOWER_API_KEY
    api_base: "https://api.flower.ai/v1"
    model_config:
      "meta/llama3.1-8b/instruct-int4":
        tokenizer_type: "huggingface"
        tokenizer: "clowman/Llama-3.1-8B-Instruct-GPTQ-Int4"
      "meta/llama3.3-70b/instruct-int4":
        tokenizer_type: "huggingface"
        tokenizer: "Satwik11/Llama-3.3-70B-Instruct-AutoRound-GPTQ-4bit"
        truncate: true
        max_tokens: 1500

  ollama:
    api_key_env: OLLAMA_API_KEY
    api_base: "http://localhost:11434/v1"
    model_config:
      "TheBloke/Mistral-7B-Instruct-v0.2-AWQ":
        uri: "http://localhost:8000/v1/chat/completions"
        temperature: 0.0
        max_tokens: 1000
        tokenizer_type: "huggingface"
        tokenizer: "mistralai/Mistral-7B-Instruct-v0.2"
      "Qwen/Qwen3-0.6B-GGUF":
        uri: "http://localhost:8000/v1/chat/completions"
        temperature: 0.0
        max_tokens: 1000
        tokenizer_type: "huggingface"
        tokenizer: "Qwen/Qwen3-0.6B-GGUF"
      "google/gemma-3-1b-it-qat-q4_0-gguf":
        uri: "http://localhost:8000/v1/chat/completions"
        temperature: 0.0
        max_tokens: 1000
        tokenizer_type: "huggingface"
        tokenizer: "google/gemma-3-1b-it-qat-q4_0-gguf"
      "Qwen/Qwen3-4B-Thinking-2507":
        uri: "http://localhost:8000/v1/chat/completions"
        temperature: 0.0
        max_tokens: 1000
        tokenizer_type: "huggingface"
        tokenizer: "Qwen/Qwen3-4B-Thinking-2507"
